{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi Variable Linear Regressoion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H(x1,x2,x3,x4 ... , x n) = w1x1 + w2x2 + w3x3 + ... wnxn + b H(x) = hypothesis\n",
    "    => Using matrix to make easy\n",
    "    \n",
    "    ex)\n",
    "        (x1,x2,x3) * (w1)  = (x1w1 + w2x2 + w3x3)\n",
    "                     (w2)                         \n",
    "                     (w3)\n",
    "                     \n",
    "       ***** H(X) = XW, X는 xi의 행렬, W는 wi의 행렬 *****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(0)\n",
    "\n",
    "X1 = [1, 0, 3, 0, 5]\n",
    "X2 = [0, 2, 0, 4, 0]\n",
    "Y  = [1, 2, 3, 4, 5]\n",
    "\n",
    "W1 = tf.Variable(tf.random_uniform([1], -10.0, 10.0))\n",
    "W2 = tf.Variable(tf.random_uniform([1], -10.0, 10.0))\n",
    "b = tf.Variable(tf.random_uniform([1], -10.0, 10.0))\n",
    "\n",
    "alpha = tf.Variable(0.001) # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 | 335.280823 |    -4.0663 |     1.1220 |  -6.065215\n",
      "   50 |  76.037262 |    -0.8001 |     1.6209 |  -4.978779\n",
      "  100 |  18.959265 |     0.7151 |     1.8781 |  -4.429109\n",
      "  150 |   6.310240 |     1.4125 |     2.0104 |  -4.134423\n",
      "  200 |   3.445082 |     1.7284 |     2.0768 |  -3.961648\n",
      "  250 |   2.743659 |     1.8667 |     2.1075 |  -3.847750\n",
      "  300 |   2.525401 |     1.9225 |     2.1184 |  -3.762738\n",
      "  350 |   2.417754 |     1.9402 |     2.1181 |  -3.692262\n",
      "  400 |   2.337300 |     1.9403 |     2.1114 |  -3.629400\n",
      "  450 |   2.264998 |     1.9325 |     2.1008 |  -3.570778\n",
      "  500 |   2.196329 |     1.9213 |     2.0881 |  -3.514729\n",
      "  550 |   2.130126 |     1.9085 |     2.0741 |  -3.460409\n",
      "  600 |   2.066037 |     1.8953 |     2.0595 |  -3.407385\n",
      "  650 |   2.003917 |     1.8819 |     2.0444 |  -3.355424\n",
      "  700 |   1.943679 |     1.8686 |     2.0293 |  -3.304398\n",
      "  750 |   1.885258 |     1.8555 |     2.0141 |  -3.254230\n",
      "  800 |   1.828595 |     1.8425 |     1.9990 |  -3.204873\n",
      "  850 |   1.773636 |     1.8297 |     1.9841 |  -3.156293\n",
      "  900 |   1.720330 |     1.8171 |     1.9693 |  -3.108468\n",
      "  950 |   1.668625 |     1.8048 |     1.9547 |  -3.061379\n",
      " 1000 |   1.618474 |     1.7926 |     1.9403 |  -3.015011\n"
     ]
    }
   ],
   "source": [
    "for i in range(1001):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hytpothesis = W1*X1 + W2*X2 + b\n",
    "        cost = tf.reduce_mean(tf.square(hytpothesis - Y))\n",
    "        \n",
    "    w1_grad, w2_grad, b_grad = tape.gradient(cost, [W1, W2, b])\n",
    "    W1.assign_sub(alpha * w1_grad)\n",
    "    W2.assign_sub(alpha * w2_grad)\n",
    "    b.assign_sub(alpha * b_grad)\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "         print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}\".format(\n",
    "          i, cost.numpy(), W1.numpy()[0], W2.numpy()[0], b.numpy()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with changing two x data as matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 | 945.217957 |    -6.3573 |    -3.5800 |  -9.671742\n",
      "   50 | 238.450104 |    -1.5414 |    -1.2255 |  -7.575880\n",
      "  100 |  67.532822 |     0.6431 |     0.1990 |  -6.463256\n",
      "  150 |  23.297581 |     1.6165 |     1.0702 |  -5.842977\n",
      "  200 |  10.697479 |     2.0365 |     1.6062 |  -5.474151\n",
      "  250 |   6.655623 |     2.2063 |     1.9358 |  -5.236833\n",
      "  300 |   5.173168 |     2.2648 |     2.1368 |  -5.070220\n",
      "  350 |   4.540474 |     2.2751 |     2.2571 |  -4.942834\n",
      "  400 |   4.217037 |     2.2654 |     2.3262 |  -4.837987\n",
      "  450 |   4.014714 |     2.2480 |     2.3627 |  -4.746597\n",
      "  500 |   3.862951 |     2.2281 |     2.3786 |  -4.663585\n",
      "  550 |   3.733929 |     2.2079 |     2.3813 |  -4.586038\n",
      "  600 |   3.616310 |     2.1881 |     2.3756 |  -4.512239\n",
      "  650 |   3.505363 |     2.1689 |     2.3647 |  -4.441153\n",
      "  700 |   3.399061 |     2.1504 |     2.3504 |  -4.372144\n",
      "  750 |   3.296504 |     2.1324 |     2.3341 |  -4.304814\n",
      "  800 |   3.197261 |     2.1148 |     2.3165 |  -4.238905\n",
      "  850 |   3.101101 |     2.0976 |     2.2983 |  -4.174256\n",
      "  900 |   3.007867 |     2.0808 |     2.2798 |  -4.110748\n",
      "  950 |   2.917454 |     2.0643 |     2.2611 |  -4.048308\n",
      " 1000 |   2.829765 |     2.0481 |     2.2425 |  -3.986883\n"
     ]
    }
   ],
   "source": [
    "X = [\n",
    "    [1., 0., 3., 0., 5.],\n",
    "    [0., 2., 0., 4., 0.]\n",
    "]\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([1, 2], -10.0, 10.0))\n",
    "b = tf.Variable(tf.random_uniform([1], -10.0, 10.0))\n",
    "\n",
    "for i in range(1001):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hytpothesis = tf.matmul(W, X) + b\n",
    "        cost = tf.reduce_mean(tf.square(hytpothesis - Y))\n",
    "        \n",
    "    w_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "    W.assign_sub(alpha * w_grad)\n",
    "    b.assign_sub(alpha * b_grad)\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "         print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}\".format(\n",
    "          i, cost.numpy(), W.numpy()[0][0], W.numpy()[0][1], b.numpy()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_uniform([1, 2], -10.0, 10.0))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 |  86.881142 |    -2.0113 |     3.3047\n",
      "   50 |  25.612244 |    -0.4880 |     2.5424\n",
      "  100 |   8.223758 |     0.2647 |     2.0323\n",
      "  150 |   2.890339 |     0.6367 |     1.6908\n",
      "  200 |   1.100923 |     0.8205 |     1.4623\n",
      "  250 |   0.445813 |     0.9113 |     1.3094\n",
      "  300 |   0.188130 |     0.9562 |     1.2071\n",
      "  350 |   0.081442 |     0.9783 |     1.1386\n",
      "  400 |   0.035788 |     0.9893 |     1.0927\n",
      "  450 |   0.015861 |     0.9947 |     1.0621\n",
      "  500 |   0.007063 |     0.9974 |     1.0415\n",
      "  550 |   0.003153 |     0.9987 |     1.0278\n",
      "  600 |   0.001410 |     0.9994 |     1.0186\n",
      "  650 |   0.000631 |     0.9997 |     1.0125\n",
      "  700 |   0.000282 |     0.9998 |     1.0083\n",
      "  750 |   0.000126 |     0.9999 |     1.0056\n",
      "  800 |   0.000057 |     1.0000 |     1.0037\n",
      "  850 |   0.000025 |     1.0000 |     1.0025\n",
      "  900 |   0.000011 |     1.0000 |     1.0017\n",
      "  950 |   0.000005 |     1.0000 |     1.0011\n",
      " 1000 |   0.000002 |     1.0000 |     1.0007\n"
     ]
    }
   ],
   "source": [
    "for i in range(1001):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hytpothesis = tf.matmul(W, X)\n",
    "        cost = tf.reduce_mean(tf.square(hytpothesis - Y))\n",
    "        \n",
    "    grads = tape.gradient(cost, [W])\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, [W]))\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "         print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f}\".format(\n",
    "          i, cost.numpy(), W.numpy()[0][0], W.numpy()[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example with using numpy module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([\n",
    "    # X1,   X2,    X3,   y\n",
    "    [ 73.,  80.,  75., 152. ],\n",
    "    [ 93.,  88.,  93., 185. ],\n",
    "    [ 89.,  91.,  90., 180. ],\n",
    "    [ 96.,  98., 100., 196. ],\n",
    "    [ 73.,  66.,  70., 142. ]\n",
    "], dtype=np.float32)\n",
    "\n",
    "# slice data\n",
    "X = data[:, :-1]\n",
    "Y = data[:, [-1]]\n",
    "\n",
    "alpha = 0.000001 # learning rate\n",
    "W = tf.Variable(tf.random_normal([3, 1]))\n",
    "optimizer = tf.train.GradientDescentOptimizer(alpha)\n",
    "\n",
    "def predict(N):\n",
    "    return tf.matmul(N, W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2.750645 |     1.8108 |     0.5732 |    -0.3612\n",
      "  2.746948 |     1.8104 |     0.5735 |    -0.3612\n",
      "  2.743236 |     1.8101 |     0.5738 |    -0.3612\n",
      "  2.739543 |     1.8098 |     0.5741 |    -0.3611\n",
      "  2.735844 |     1.8095 |     0.5744 |    -0.3611\n",
      "  2.732152 |     1.8092 |     0.5747 |    -0.3610\n",
      "  2.728463 |     1.8088 |     0.5749 |    -0.3610\n",
      "  2.724800 |     1.8085 |     0.5752 |    -0.3610\n",
      "  2.721153 |     1.8082 |     0.5755 |    -0.3609\n",
      "  2.717521 |     1.8079 |     0.5758 |    -0.3609\n",
      "  2.713911 |     1.8076 |     0.5761 |    -0.3609\n",
      "  2.710268 |     1.8072 |     0.5764 |    -0.3608\n",
      "  2.706658 |     1.8069 |     0.5766 |    -0.3608\n",
      "  2.703096 |     1.8066 |     0.5769 |    -0.3607\n",
      "  2.699516 |     1.8063 |     0.5772 |    -0.3607\n",
      "  2.695921 |     1.8060 |     0.5775 |    -0.3607\n",
      "  2.692348 |     1.8057 |     0.5777 |    -0.3606\n",
      "  2.688811 |     1.8054 |     0.5780 |    -0.3606\n",
      "  2.685214 |     1.8050 |     0.5783 |    -0.3606\n",
      "  2.681655 |     1.8047 |     0.5786 |    -0.3605\n",
      "  2.678124 |     1.8044 |     0.5789 |    -0.3605\n",
      "  2.674572 |     1.8041 |     0.5791 |    -0.3604\n",
      "  2.671038 |     1.8038 |     0.5794 |    -0.3604\n",
      "  2.667499 |     1.8035 |     0.5797 |    -0.3604\n",
      "  2.664008 |     1.8031 |     0.5800 |    -0.3603\n",
      "  2.660497 |     1.8028 |     0.5802 |    -0.3603\n",
      "  2.656986 |     1.8025 |     0.5805 |    -0.3602\n",
      "  2.653516 |     1.8022 |     0.5808 |    -0.3602\n",
      "  2.650075 |     1.8019 |     0.5811 |    -0.3602\n",
      "  2.646619 |     1.8016 |     0.5813 |    -0.3601\n",
      "  2.643178 |     1.8013 |     0.5816 |    -0.3601\n",
      "  2.639746 |     1.8010 |     0.5819 |    -0.3601\n",
      "  2.636326 |     1.8007 |     0.5821 |    -0.3600\n",
      "  2.632904 |     1.8003 |     0.5824 |    -0.3600\n",
      "  2.629488 |     1.8000 |     0.5827 |    -0.3599\n",
      "  2.626068 |     1.7997 |     0.5829 |    -0.3599\n",
      "  2.622649 |     1.7994 |     0.5832 |    -0.3599\n",
      "  2.619270 |     1.7991 |     0.5835 |    -0.3598\n",
      "  2.615861 |     1.7988 |     0.5838 |    -0.3598\n",
      "  2.612466 |     1.7985 |     0.5840 |    -0.3597\n",
      "  2.609084 |     1.7982 |     0.5843 |    -0.3597\n"
     ]
    }
   ],
   "source": [
    "for i in range(2001):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = predict(X)\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "        \n",
    "    grads = tape.gradient(cost, [W])\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, [W]))\n",
    "    if i % 50 == 0:\n",
    "        print(\"{:10.6f} | {:10.4f} | {:10.4f} | {:10.4f}\".format(cost, W[0][0], W[1][0], W[2][0]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=347099, shape=(5, 1), dtype=float32, numpy=\n",
       "array([[151.03267],\n",
       "       [185.19592],\n",
       "       [180.8352 ],\n",
       "       [193.91547],\n",
       "       [144.65108]], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(X) # 예상 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[152.],\n",
       "       [185.],\n",
       "       [180.],\n",
       "       [196.],\n",
       "       [142.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y # 실제 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[182.45297],\n",
       "       [174.22711]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 새로운 데이터에 대한 예측\n",
    "\n",
    "predict([[ 89.,  95.,  92.],[ 84.,  92.,  85.]]).numpy() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
